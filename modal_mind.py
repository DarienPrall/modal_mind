#----------------------------------------------------------------------
#
# PROJECT INFORMATION
# Title: ModalMind
# Author: Darien Prall
# Start Date: 01-04-2025 (mmddyyyy)
# End Date: 
# Project Description: For this project, I explore the usage of modal verbs in different texts from the Gutenberg Corpus. I will analyze the relative frequency through different Natural Language Processing (NLP) techniques.
#
# PROJECT STEPS
# Task 0: Import and Clean Data
#
# Task 1: Modals in the Gutenberg Corpus
# - Task 1.1: Install NLTK
# - Task 1.2: Download the Gutenberg Corpus
# - Task 1.3: Define Each Modal Group
# - Task 1.4: Count Relative Frequencies of Each Modal Verb
# - Task 1.5: Find the Texts with the Largest Span of Modal Frequencies
# - Task 1.6: Compare Usage of Modals in the Two Texts
#
# Task 2: Inaugural Corpus - Analyzing Kennedy's 1961 Speech
# - Task 2.1: Download the Inaugural Corpus
# - Task 2.2: Load the 1961 Kennedy Speech
# - Task 2.3: Identify the 10 Most Frequent Used Long Words
# - Task 2.4: Use WordNet to Find Synonyms and Hyponyms
# - Task 2.5: Reflect of the Results
#----------------------------------------------------------------------

#----------------------------------------------------------------------
# Task 1: Modals in the Gutenberg Corpus

# - Task 1.1: Install NLTK
# What this code is doing: Importing the NLTK library to allow the download of the gutenberg texts
import nltk
from nltk.corpus import gutenberg

# - Task 1.2: Download the Gutenberg Corpus
# What this code is doing: Downloading the gutenberg texts and splitter. Storing all texts to a variable to print what they are
nltk.download('gutenberg')
nltk.download('punkt')

files = gutenberg.fileids()
#print(files)
# Files is an array

# - Task 1.3: Define Each Modal Group
# What this code is doing: Creating a list of modal verbs I wnat to look for within the texts (note they are all lowercase)
list_of_modals = ['can', 'could', 'may', 'might', 'will', 'would', 'should']

# - Task 1.4: Count Relative Frequencies of Each Modal Verb
# What this code is doing: 
# Creating an empty dictionary. This dictonary will be filled with the text file as the key and a dictionary of modal counts as the value
# This information is generated by the for loop that loops through each file and converts its contents to an array of arrays
# Creates another empty dictionary to store the modal as the key and the count of the modal in the text as the value.
# The dictionary of modal:count is stored as the value to frequency_of_modals_by_file
frequency_of_modals_by_file = {
# KEY  : VALUE
# FILE : {modal: count}

}
from tabulate import tabulate

for file in files:
    file_words = gutenberg.words(file)
    lowercase_file_words = []
    for word in file_words:
        lowercase_file_words.append(word.lower())
    count_of_each_modal_in_file = {
        # KEY   :  VALUE
        # modal : count of modal
    }
    for modal in list_of_modals:
        count_of_each_modal_in_file[modal] = lowercase_file_words.count(modal)
    frequency_of_modals_by_file[file] = count_of_each_modal_in_file

frequency_table = [(modal, count) for modal, count in frequency_of_modals_by_file.items()]
print(tabulate(frequency_table, headers=["Modal", "Count"], tablefmt="grid"))


# - Task 1.5: Find the Texts with the Largest Span of Modal Frequencies
# What this code is doing: 
# Creating an empty dictionary to store modal_span_information. That information will be the most_frequent_file, least_frequent__file, most_frequent_file_count, and least_frequent_file_count

modal_span_information = {
    # KEY   : VALUE
    # modal : {modal_span_information}
}
for modal in list_of_modals:
    modal_span_information[modal] = {
        "most_frequent_file": "", 
        "least_frequent_file": "", 
        "most_frequent_file_count": 0, 
        "least_frequent_file_count": float('inf')
        }

for file, dic_modal_counts in frequency_of_modals_by_file.items():
    for modal, count_of_modal in dic_modal_counts.items():
        if count_of_modal > modal_span_information[modal]["most_frequent_file_count"]:
            modal_span_information[modal]["most_frequent_file_count"] = count_of_modal
            modal_span_information[modal]["most_frequent_file"] = file
        if count_of_modal < modal_span_information[modal]["least_frequent_file_count"]:
            modal_span_information[modal]["least_frequent_file_count"] = count_of_modal
            modal_span_information[modal]["least_frequent_file"] = file

#print(modal_span_information)

# - Task 1.6: Compare Usage of Modals in the Two Texts
# What this code is doing: This code is looping through each key:value in the modal_span_information to print each of the modals. 
# It is then getting the file names of where the modal appears the most and least and then prints the results

for modal, span in modal_span_information.items():
    #print(f"Modal: {modal}")
    most_modal_count_file = gutenberg.words(span['most_frequent_file'])
    least_modal_count_file = gutenberg.words(span['least_frequent_file'])
    most_modal_count = most_modal_count_file.count(modal)
    least_modal_count = least_modal_count_file.count(modal)
    max_length = len(most_modal_count_file)
    min_length = len(least_modal_count_file)
    print(f" Appears most frequent in {span['most_frequent_file']}:\n  {most_modal_count} times out of {max_length} total words. Relative Frequency = {most_modal_count / max_length:.6f}")
    print(f" Appears the least frequent in {span['least_frequent_file']}:\n  {least_modal_count} times out of {min_length} total words. Relative Frequency = {least_modal_count / min_length:.6f}")

table_data = []
headers = ["Modal", "Most Frequent File", "Most Frequent Count", "Most Frequent Total Words", "Most Frequent Frequency", 
           "Least Frequent File", "Least Frequent Count", "Least Frequent Total Words", "Least Frequent Frequency"]


for modal, span in modal_span_information.items():
    most_modal_count_file = gutenberg.words(span['most_frequent_file'])
    least_modal_count_file = gutenberg.words(span['least_frequent_file'])
    
    most_modal_count = most_modal_count_file.count(modal)
    least_modal_count = least_modal_count_file.count(modal)
    
    max_length = len(most_modal_count_file)
    min_length = len(least_modal_count_file)
    
    most_modal_frequency = most_modal_count / max_length
    least_modal_frequency = least_modal_count / min_length
    
    table_data.append([
        modal, 
        span['most_frequent_file'], 
        most_modal_count, 
        max_length, 
        f"{most_modal_frequency:.6f}",
        span['least_frequent_file'], 
        least_modal_count, 
        min_length, 
        f"{least_modal_frequency:.6f}"
    ])

print(tabulate(table_data, headers=headers, tablefmt="grid"))

# Explaination of Why the Modal Counts are Different Between Two Texts:
# Setting aside that the books are different, as this is not a good enough explaination of why texts are different across books, its better to look at the time and tone of the texts.
# Formality: Some texts may be more formal than others so woulds like 'may' and 'shall' would appear more. The author may be indicating obligation or necessity.
# Type of Storytelling: The story could be a narrative or a descriptive text in which hypothetical and conditional modals are used more. These tend to explore potential outcomes of character choices.
# Audience: Depending on who the text is written for could could determine how casual modals are used like 'may' and 'could' that fit a more conversational tone.
# Technicality: If the text is in the form of a techincal document, it may focus on precision and clarity with modals like "must"
#----------------------------------------------------------------------

#----------------------------------------------------------------------
# Task 2: Inaugural Corpus - Analyzing Kennedy's 1961 Speech

# - Task 2.1: Download the Inaugural Corpus
# What this code is doing:
from nltk.corpus import inaugural
nltk.download('inaugural')

# - Task 2.2: Load the 1961 Kennedy Speech
# What this code is doing: Getting all of the files in inaugural so I can see the name of the Kenndey Speech file (even though it was provided). It grabs the contents of the speech and then splits its contents into words.
all_files = inaugural.fileids()
#for file in all_files:
#    print(file)

kennedys_speech = inaugural.words('1961-Kennedy.txt')

# - Task 2.3: Identify the 10 Most Frequent Used Long Words
# What this code is doing: This code is looking for any words that are over 7-characters in length and appending them to an array. The words then need to be added to a dictionary with the word being the key and the count being the value. The dictionary can then be sorted to get the top 10 long words

long_words = []
for word in kennedys_speech:
    if len(word) > 7:
        long_words.append(word.lower())

frequency_of_long_words = {}

for word in long_words:
    if word in frequency_of_long_words:
        frequency_of_long_words[word] += 1
    else:
        frequency_of_long_words[word] = 1

sorted_long_words_frequency = sorted(frequency_of_long_words.items(), key = lambda x: x[1], reverse = True)
top_10_long_words = sorted_long_words_frequency[:10]
print(f"The top ten long words in Kennedy's 1961 Speech are: {top_10_long_words}")

# - Task 2.4: Use WordNet to Find Synonyms and Hyponyms
# What this code is doing: 
from nltk.corpus import wordnet as wn
nltk.download('wordnet')

for word, _ in top_10_long_words:
    synonyms = []
    hyponyms = []
    
    for synset in wn.synsets(word):

        for lemma in synset.lemma_names():
            if lemma not in synonyms:
                synonyms.append(lemma)
        
        for hypo in synset.hyponyms():
            for lemma in hypo.lemma_names():
                if lemma not in hyponyms:
                    hyponyms.append(lemma)
    
    print(f"Word: {word}")
    print(f"  Synonyms: {', '.join(synonyms)}")
    print(f"  Hyponyms: {', '.join(hyponyms)}")
    print()
