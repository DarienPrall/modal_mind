{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Project Information**\n",
    "Title: Assessment_9: NLP AI Applications\n",
    "Author: Darien Prall\n",
    "Date: 01-04-2024 (mmddyyyy)\n",
    "\n",
    "**Project Description**\n",
    "For this project, I explore the usage of modal verbs in different texts from the Gutenberg Corpus. I will analyze the relative frequency through different Natural Language Processing (NLP) techniques.\n",
    "\n",
    "**Project Steps**\n",
    "Task 1: Modals in the Gutenberg Corpus\n",
    "- Task 1.1: Install NLTK\n",
    "- Task 1.2: Download the Gutenberg Corpus\n",
    "- Task 1.3: Define Each Modal Group\n",
    "- Task 1.4: Count Relative Frequencies of Each Modal Verb\n",
    "- Task 1.5: Find the Texts with the Largest Span of Modal Frequencies\n",
    "- Task 1.6: Compare Usage of Modals in the Two Texts\n",
    "\n",
    "Task 2: Inaugural Corpus - Analyzing Kennedy's 1961 Speech\n",
    "- Task 2.1: Download the Inaugural Corpus\n",
    "- Task 2.2: Load the 1961 Kennedy Speech\n",
    "- Task 2.3: Identify the 10 Most Frequent Used Long Words\n",
    "- Task 2.4: Use WordNet to Find Synonyms and Hyponyms\n",
    "- Task 2.5: Reflect of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Task 1: Modals in the Gutenberg Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.1: Install NLTK**\n",
    "**What this code is doing:** Importing the NLTK library to allow the download of the gutenberg texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.2: Download the Gutenberg Corpus**\n",
    "**What this code is doing:** Downloading the gutenberg texts and splitter. Storing all texts to a variable to print what they are. Then checking the title of each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = gutenberg.fileids()\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.3: Define Each Modal Group**\n",
    "**What this code is doing:** Creating a list of modal verbs I want to look for within the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_modals = ['can', 'could', 'may', 'might', 'will', 'would', 'should']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.4: Count Relative Frequencies of Each Modal Verb**\n",
    "**What this code is doing:** Creating an empty dictionary. This dictonary will be filled with the text file as the key and a dictionary of modal counts as the value. This information is generated by the for loop that loops through each file and converts its contents to an array of arrays. Creates another empty dictionary to store the modal as the key and the count of the modal in the text as the value. The dictionary of modal:count is stored as the value to frequency_of_modals_by_file. For best practices, I like to comment the key:value pair of the dictionary being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_of_modals_by_file = {\n",
    "# KEY  : VALUE\n",
    "# FILE : {modal: count}\n",
    "\n",
    "}\n",
    "\n",
    "for file in files:\n",
    "    file_words = gutenberg.words(file)\n",
    "    lowercase_file_words = []\n",
    "    for word in file_words:\n",
    "        lowercase_file_words.append(word.lower())\n",
    "    count_of_each_modal_in_file = {\n",
    "        # KEY   :  VALUE\n",
    "        # modal : count of modal\n",
    "    }\n",
    "    for modal in list_of_modals:\n",
    "        count_of_each_modal_in_file[modal] = lowercase_file_words.count(modal)\n",
    "    frequency_of_modals_by_file[file] = count_of_each_modal_in_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.5: Find the Texts with the Largest Span of Modal Frequencies**\n",
    "**What this code is doing:** Creating an empty dictionary to store modal_span_information. That information will be the most_frequent_file, least_frequent__file, most_frequent_file_count, and least_frequent_file_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_span_information = {\n",
    "    # KEY   : VALUE\n",
    "    # modal : {modal_span_information}\n",
    "}\n",
    "for modal in list_of_modals:\n",
    "    modal_span_information[modal] = {\n",
    "        \"most_frequent_file\": \"\", \n",
    "        \"least_frequent_file\": \"\", \n",
    "        \"most_frequent_file_count\": 0, \n",
    "        \"least_frequent_file_count\": float('inf')\n",
    "        }\n",
    "\n",
    "for file, dic_modal_counts in frequency_of_modals_by_file.items():\n",
    "    for modal, count_of_modal in dic_modal_counts.items():\n",
    "        if count_of_modal > modal_span_information[modal][\"most_frequent_file_count\"]:\n",
    "            modal_span_information[modal][\"most_frequent_file_count\"] = count_of_modal\n",
    "            modal_span_information[modal][\"most_frequent_file\"] = file\n",
    "        if count_of_modal < modal_span_information[modal][\"least_frequent_file_count\"]:\n",
    "            modal_span_information[modal][\"least_frequent_file_count\"] = count_of_modal\n",
    "            modal_span_information[modal][\"least_frequent_file\"] = file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Compare Usage of Modals in the Two Texts**\n",
    "**What this code is doing:** This code is looping through each key:value in the modal_span_information to print each of the modals. \n",
    "It is then getting the file names of where the modal appears the most and least and then prints the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for modal, span in modal_span_information.items():\n",
    "    print(f\"Modal: {modal}\")\n",
    "    most_modal_count_file = gutenberg.words(span['most_frequent_file'])\n",
    "    least_modal_count_file = gutenberg.words(span['least_frequent_file'])\n",
    "    most_modal_count = most_modal_count_file.count(modal)\n",
    "    least_modal_count = least_modal_count_file.count(modal)\n",
    "    max_length = len(most_modal_count_file)\n",
    "    min_length = len(least_modal_count_file)\n",
    "    print(f\" Appears most frequent in {span['most_frequent_file']}:\\n  {most_modal_count} times out of {max_length} total words. Relative Frequency = {most_modal_count / max_length:.6f}\")\n",
    "    print(f\" Appears the least frequent in {span['least_frequent_file']}:\\n  {least_modal_count} times out of {min_length} total words. Relative Frequency = {least_modal_count / min_length:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Task 2: Inaugural Corpus - Analyzing Kennedy's 1961 Speech**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.1: Download the Inaugural Corpus**\n",
    "**What this code is doing:** Simply importing and downloading the inaugural module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.2: Load the 1961 Kennedy Speech**\n",
    "**What this code is doing:** Getting all of the files in inaugural so I can see the name of the Kenndey Speech file (even though it was provided). It grabs the contents of the speech and then splits its contents into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = inaugural.fileids()\n",
    "#for file in all_files:\n",
    "#    print(file)\n",
    "\n",
    "kennedys_speech = inaugural.words('1961-Kennedy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.3: Identify the 10 Most Frequent Used Long Words**\n",
    "**What this code is doing:** This code is looking for any words that are over 7-characters in length and appending them to an array. The words then need to be added to a dictionary with the word being the key and the count being the value. The dictionary can then be sorted to get the top 10 long words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = []\n",
    "for word in kennedys_speech:\n",
    "    if len(word) > 7:\n",
    "        long_words.append(word.lower())\n",
    "\n",
    "frequency_of_long_words = {}\n",
    "\n",
    "for word in long_words:\n",
    "    if word in frequency_of_long_words:\n",
    "        frequency_of_long_words[word] += 1\n",
    "    else:\n",
    "        frequency_of_long_words[word] = 1\n",
    "\n",
    "sorted_long_words_frequency = sorted(frequency_of_long_words.items(), key = lambda x: x[1], reverse = True)\n",
    "top_10_long_words = sorted_long_words_frequency[:10]\n",
    "print(f\"The top ten long words in Kennedy's 1961 Speech are: {top_10_long_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.4: Use WordNet to Find Synonyms and Hyponyms**\n",
    "**What this code is doing:** This code is importing and downloading the wordnet from nltk. It then loops through the top_10_long words then extracts any unique synonyms and hyponyms for each word. These are stored in two seperate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, _ in top_10_long_words:\n",
    "    synonyms = []\n",
    "    hyponyms = []\n",
    "    \n",
    "    for synset in wn.synsets(word):\n",
    "\n",
    "        for lemma in synset.lemma_names():\n",
    "            if lemma not in synonyms:\n",
    "                synonyms.append(lemma)\n",
    "        \n",
    "        for hypo in synset.hyponyms():\n",
    "            for lemma in hypo.lemma_names():\n",
    "                if lemma not in hyponyms:\n",
    "                    hyponyms.append(lemma)\n",
    "    \n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"  Synonyms: {', '.join(synonyms)}\")\n",
    "    print(f\"  Hyponyms: {', '.join(hyponyms)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Task 2.5: Reflect of the Results##\n",
    "I noticed that for words like 'powerful' and 'themselves' that it did not return any hypoymns. 'Themselves' also didnt return any synonyms. At first I thought this was error in the code but it turns out they are not in the hierarchy of WordNet.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
