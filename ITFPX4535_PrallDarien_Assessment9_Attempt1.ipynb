{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Project Information**\n",
    "Title: Assessment_9: NLP AI Applications\n",
    "Author: Darien Prall\n",
    "Date: 01-04-2024 (mmddyyyy)\n",
    "\n",
    "**Project Description**\n",
    "For this project, I explore the usage of modal verbs in different texts from the Gutenberg Corpus. I will analyze the relative frequency through different Natural Language Processing (NLP) techniques.\n",
    "\n",
    "**Project Steps**\n",
    "Task 1: Modals in the Gutenberg Corpus\n",
    "- Task 1.1: Install NLTK\n",
    "- Task 1.2: Download the Gutenberg Corpus\n",
    "- Task 1.3: Define Each Modal Group\n",
    "- Task 1.4: Count Relative Frequencies of Each Modal Verb\n",
    "- Task 1.5: Find the Texts with the Largest Span of Modal Frequencies\n",
    "- Task 1.6: Compare Usage of Modals in the Two Texts\n",
    "\n",
    "Task 2: Inaugural Corpus - Analyzing Kennedy's 1961 Speech\n",
    "- Task 2.1: Download the Inaugural Corpus\n",
    "- Task 2.2: Load the 1961 Kennedy Speech\n",
    "- Task 2.3: Identify the 10 Most Frequent Used Long Words\n",
    "- Task 2.4: Use WordNet to Find Synonyms and Hyponyms\n",
    "- Task 2.5: Reflect of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Task 1: Modals in the Gutenberg Corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.1: Install NLTK**\n",
    "**What this code is doing:** Importing the NLTK library to allow the download of the gutenberg texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.2: Download the Gutenberg Corpus**\n",
    "**What this code is doing:** Downloading the gutenberg texts and splitter. Storing all texts to a variable to print what they are. Then checking the title of each file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "austen-emma.txt\n",
      "austen-persuasion.txt\n",
      "austen-sense.txt\n",
      "bible-kjv.txt\n",
      "blake-poems.txt\n",
      "bryant-stories.txt\n",
      "burgess-busterbrown.txt\n",
      "carroll-alice.txt\n",
      "chesterton-ball.txt\n",
      "chesterton-brown.txt\n",
      "chesterton-thursday.txt\n",
      "edgeworth-parents.txt\n",
      "melville-moby_dick.txt\n",
      "milton-paradise.txt\n",
      "shakespeare-caesar.txt\n",
      "shakespeare-hamlet.txt\n",
      "shakespeare-macbeth.txt\n",
      "whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "files = gutenberg.fileids()\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.3: Define Each Modal Group**\n",
    "**What this code is doing:** Creating a list of modal verbs I want to look for within the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_modals = ['can', 'could', 'may', 'might', 'will', 'would', 'should']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.4: Count Relative Frequencies of Each Modal Verb**\n",
    "**What this code is doing:** Creating an empty dictionary. This dictonary will be filled with the text file as the key and a dictionary of modal counts as the value. This information is generated by the for loop that loops through each file and converts its contents to an array of arrays. Creates another empty dictionary to store the modal as the key and the count of the modal in the text as the value. The dictionary of modal:count is stored as the value to frequency_of_modals_by_file. For best practices, I like to comment the key:value pair of the dictionary being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| Modal                   | Count                                                                                            |\n",
      "+=========================+==================================================================================================+\n",
      "| austen-emma.txt         | {'can': 284, 'could': 837, 'may': 221, 'might': 326, 'will': 570, 'would': 820, 'should': 369}   |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| austen-persuasion.txt   | {'can': 107, 'could': 451, 'may': 87, 'might': 166, 'will': 167, 'would': 355, 'should': 188}    |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| austen-sense.txt        | {'can': 218, 'could': 578, 'may': 175, 'might': 215, 'will': 363, 'would': 515, 'should': 236}   |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| bible-kjv.txt           | {'can': 235, 'could': 166, 'may': 1027, 'might': 475, 'will': 3836, 'would': 451, 'should': 783} |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| blake-poems.txt         | {'can': 28, 'could': 6, 'may': 6, 'might': 2, 'will': 3, 'would': 5, 'should': 6}                |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| bryant-stories.txt      | {'can': 78, 'could': 158, 'may': 22, 'might': 23, 'will': 147, 'would': 112, 'should': 38}       |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| burgess-busterbrown.txt | {'can': 24, 'could': 56, 'may': 3, 'might': 17, 'will': 20, 'would': 46, 'should': 13}           |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| carroll-alice.txt       | {'can': 63, 'could': 77, 'may': 13, 'might': 28, 'will': 33, 'would': 83, 'should': 27}          |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| chesterton-ball.txt     | {'can': 143, 'could': 117, 'may': 96, 'might': 72, 'will': 203, 'would': 140, 'should': 75}      |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| chesterton-brown.txt    | {'can': 129, 'could': 171, 'may': 48, 'might': 72, 'will': 117, 'would': 135, 'should': 56}      |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| chesterton-thursday.txt | {'can': 122, 'could': 151, 'may': 60, 'might': 73, 'will': 122, 'would': 120, 'should': 54}      |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| edgeworth-parents.txt   | {'can': 359, 'could': 426, 'may': 188, 'might': 127, 'will': 556, 'would': 511, 'should': 274}   |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| melville-moby_dick.txt  | {'can': 236, 'could': 216, 'may': 240, 'might': 183, 'will': 391, 'would': 432, 'should': 183}   |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| milton-paradise.txt     | {'can': 129, 'could': 67, 'may': 126, 'might': 107, 'will': 183, 'would': 58, 'should': 65}      |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| shakespeare-caesar.txt  | {'can': 19, 'could': 18, 'may': 38, 'might': 13, 'will': 163, 'would': 44, 'should': 42}         |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| shakespeare-hamlet.txt  | {'can': 35, 'could': 31, 'may': 65, 'might': 30, 'will': 149, 'would': 73, 'should': 56}         |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| shakespeare-macbeth.txt | {'can': 26, 'could': 16, 'may': 35, 'might': 8, 'will': 72, 'would': 53, 'should': 42}           |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n",
      "| whitman-leaves.txt      | {'can': 92, 'could': 52, 'may': 99, 'might': 26, 'will': 273, 'would': 93, 'should': 43}         |\n",
      "+-------------------------+--------------------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "frequency_of_modals_by_file = {\n",
    "# KEY  : VALUE\n",
    "# FILE : {modal: count}\n",
    "\n",
    "}\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "for file in files:\n",
    "    file_words = gutenberg.words(file)\n",
    "    lowercase_file_words = []\n",
    "    for word in file_words:\n",
    "        lowercase_file_words.append(word.lower())\n",
    "    count_of_each_modal_in_file = {\n",
    "        # KEY   :  VALUE\n",
    "        # modal : count of modal\n",
    "    }\n",
    "    for modal in list_of_modals:\n",
    "        count_of_each_modal_in_file[modal] = lowercase_file_words.count(modal)\n",
    "    frequency_of_modals_by_file[file] = count_of_each_modal_in_file\n",
    "\n",
    "# Printing the results in table format\n",
    "frequency_table = [(modal, count) for modal, count in frequency_of_modals_by_file.items()]\n",
    "print(tabulate(frequency_table, headers=[\"Modal\", \"Count\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 1.5: Find the Texts with the Largest Span of Modal Frequencies**\n",
    "**What this code is doing:** Creating an empty dictionary to store modal_span_information. That information will be the most_frequent_file, least_frequent__file, most_frequent_file_count, and least_frequent_file_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modal_span_information = {\n",
    "    # KEY   : VALUE\n",
    "    # modal : {modal_span_information}\n",
    "}\n",
    "for modal in list_of_modals:\n",
    "    modal_span_information[modal] = {\n",
    "        \"most_frequent_file\": \"\", \n",
    "        \"least_frequent_file\": \"\", \n",
    "        \"most_frequent_file_count\": 0, \n",
    "        \"least_frequent_file_count\": float('inf')\n",
    "        }\n",
    "\n",
    "for file, dic_modal_counts in frequency_of_modals_by_file.items():\n",
    "    for modal, count_of_modal in dic_modal_counts.items():\n",
    "        if count_of_modal > modal_span_information[modal][\"most_frequent_file_count\"]:\n",
    "            modal_span_information[modal][\"most_frequent_file_count\"] = count_of_modal\n",
    "            modal_span_information[modal][\"most_frequent_file\"] = file\n",
    "        if count_of_modal < modal_span_information[modal][\"least_frequent_file_count\"]:\n",
    "            modal_span_information[modal][\"least_frequent_file_count\"] = count_of_modal\n",
    "            modal_span_information[modal][\"least_frequent_file\"] = file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Compare Usage of Modals in the Two Texts**\n",
    "**What this code is doing:** This code is looping through each key:value in the modal_span_information to print each of the modals. \n",
    "It is then getting the file names of where the modal appears the most and least and then prints the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| Modal   | Most Frequent File    |   Most Frequent Count |   Most Frequent Total Words |   Most Frequent Frequency | Least Frequent File     |   Least Frequent Count |   Least Frequent Total Words |   Least Frequent Frequency |\n",
      "+=========+=======================+=======================+=============================+===========================+=========================+========================+==============================+============================+\n",
      "| can     | edgeworth-parents.txt |                   340 |                      210663 |                  0.001614 | shakespeare-caesar.txt  |                     16 |                        25833 |                   0.000619 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| could   | austen-emma.txt       |                   825 |                      192427 |                  0.004287 | blake-poems.txt         |                      3 |                         8354 |                   0.000359 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| may     | bible-kjv.txt         |                  1024 |                     1010654 |                  0.001013 | burgess-busterbrown.txt |                      3 |                        18963 |                   0.000158 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| might   | bible-kjv.txt         |                   475 |                     1010654 |                  0.00047  | blake-poems.txt         |                      2 |                         8354 |                   0.000239 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| will    | bible-kjv.txt         |                  3807 |                     1010654 |                  0.003767 | blake-poems.txt         |                      3 |                         8354 |                   0.000359 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| would   | austen-emma.txt       |                   815 |                      192427 |                  0.004235 | blake-poems.txt         |                      3 |                         8354 |                   0.000359 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n",
      "| should  | bible-kjv.txt         |                   768 |                     1010654 |                  0.00076  | blake-poems.txt         |                      6 |                         8354 |                   0.000718 |\n",
      "+---------+-----------------------+-----------------------+-----------------------------+---------------------------+-------------------------+------------------------+------------------------------+----------------------------+\n"
     ]
    }
   ],
   "source": [
    "for modal, span in modal_span_information.items():\n",
    "    #print(f\"Modal: {modal}\")\n",
    "    most_modal_count_file = gutenberg.words(span['most_frequent_file'])\n",
    "    least_modal_count_file = gutenberg.words(span['least_frequent_file'])\n",
    "    most_modal_count = most_modal_count_file.count(modal)\n",
    "    least_modal_count = least_modal_count_file.count(modal)\n",
    "    max_length = len(most_modal_count_file)\n",
    "    min_length = len(least_modal_count_file)\n",
    "    #print(f\" Appears most frequent in {span['most_frequent_file']}:\\n  {most_modal_count} times out of {max_length} total words. Relative Frequency = {most_modal_count / max_length:.6f}\")\n",
    "    #print(f\" Appears the least frequent in {span['least_frequent_file']}:\\n  {least_modal_count} times out of {min_length} total words. Relative Frequency = {least_modal_count / min_length:.6f}\")\n",
    "\n",
    "    table_data = []\n",
    "headers = [\"Modal\", \"Most Frequent File\", \"Most Frequent Count\", \"Most Frequent Total Words\", \"Most Frequent Frequency\", \n",
    "           \"Least Frequent File\", \"Least Frequent Count\", \"Least Frequent Total Words\", \"Least Frequent Frequency\"]\n",
    "\n",
    "\n",
    "for modal, span in modal_span_information.items():\n",
    "    most_modal_count_file = gutenberg.words(span['most_frequent_file'])\n",
    "    least_modal_count_file = gutenberg.words(span['least_frequent_file'])\n",
    "    \n",
    "    most_modal_count = most_modal_count_file.count(modal)\n",
    "    least_modal_count = least_modal_count_file.count(modal)\n",
    "    \n",
    "    max_length = len(most_modal_count_file)\n",
    "    min_length = len(least_modal_count_file)\n",
    "    \n",
    "    most_modal_frequency = most_modal_count / max_length\n",
    "    least_modal_frequency = least_modal_count / min_length\n",
    "    \n",
    "    table_data.append([\n",
    "        modal, \n",
    "        span['most_frequent_file'], \n",
    "        most_modal_count, \n",
    "        max_length, \n",
    "        f\"{most_modal_frequency:.6f}\",\n",
    "        span['least_frequent_file'], \n",
    "        least_modal_count, \n",
    "        min_length, \n",
    "        f\"{least_modal_frequency:.6f}\"\n",
    "    ])\n",
    "\n",
    "print(tabulate(table_data, headers=headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**Explaination of Why the Modal Counts are Different Between Two Texts**\n",
    "Setting aside that the books are different, as this is not a good enough explaination of why texts are different across books, its better to look at the time and tone of the texts.\n",
    "**Formality**: Some texts may be more formal than others so woulds like 'may' and 'shall' would appear more. The author may be indicating obligation or necessity.\n",
    "**Type of Storytelling**: The story could be a narrative or a descriptive text in which hypothetical and conditional modals are used more. These tend to explore potential outcomes of character choices.\n",
    "**Audience**: Depending on who the text is written for could could determine how casual modals are used like 'may' and 'could' that fit a more conversational tone.\n",
    "**Technicality**: If the text is in the form of a techincal document, it may focus on precision and clarity with modals like \"must\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Task 2: Inaugural Corpus - Analyzing Kennedy's 1961 Speech**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.1: Download the Inaugural Corpus**\n",
    "**What this code is doing:** Simply importing and downloading the inaugural module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package inaugural to\n",
      "[nltk_data]     /Users/darienprall/nltk_data...\n",
      "[nltk_data]   Package inaugural is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.2: Load the 1961 Kennedy Speech**\n",
    "**What this code is doing:** Getting all of the files in inaugural so I can see the name of the Kenndey Speech file (even though it was provided). It grabs the contents of the speech and then splits its contents into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = inaugural.fileids()\n",
    "#for file in all_files:\n",
    "#    print(file)\n",
    "\n",
    "kennedys_speech = inaugural.words('1961-Kennedy.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.3: Identify the 10 Most Frequent Used Long Words**\n",
    "**What this code is doing:** This code is looking for any words that are over 7-characters in length and appending them to an array. The words then need to be added to a dictionary with the word being the key and the count being the value. The dictionary can then be sorted to get the top 10 long words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top ten long words in Kennedy's 1961 Speech are: [('citizens', 5), ('president', 4), ('americans', 4), ('generation', 3), ('forebears', 2), ('revolution', 2), ('committed', 2), ('powerful', 2), ('supporting', 2), ('themselves', 2)]\n"
     ]
    }
   ],
   "source": [
    "long_words = []\n",
    "for word in kennedys_speech:\n",
    "    if len(word) > 7:\n",
    "        long_words.append(word.lower())\n",
    "\n",
    "frequency_of_long_words = {}\n",
    "\n",
    "for word in long_words:\n",
    "    if word in frequency_of_long_words:\n",
    "        frequency_of_long_words[word] += 1\n",
    "    else:\n",
    "        frequency_of_long_words[word] = 1\n",
    "\n",
    "sorted_long_words_frequency = sorted(frequency_of_long_words.items(), key = lambda x: x[1], reverse = True)\n",
    "top_10_long_words = sorted_long_words_frequency[:10]\n",
    "print(f\"The top ten long words in Kennedy's 1961 Speech are: {top_10_long_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###**Task 2.4: Use WordNet to Find Synonyms and Hyponyms**\n",
    "**What this code is doing:** This code is importing and downloading the wordnet from nltk. It then loops through the top_10_long words then extracts any unique synonyms and hyponyms for each word. These are stored in two seperate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/darienprall/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: citizens\n",
      "  Synonyms: citizen\n",
      "  Hyponyms: voter, elector, freeman, freewoman, thane, private_citizen, active_citizen, civilian, repatriate\n",
      "\n",
      "Word: president\n",
      "  Synonyms: president, President_of_the_United_States, United_States_President, President, Chief_Executive, chairman, chairwoman, chair, chairperson, prexy\n",
      "  Hyponyms: ex-president, Kalon_Tripa, vice_chairman\n",
      "\n",
      "Word: americans\n",
      "  Synonyms: American, American_English, American_language\n",
      "  Hyponyms: New_Mexican, North_Carolinian, Tarheel, New_Englander, Yankee, Rhode_Islander, Wyomingite, Carolinian, Kentuckian, Bluegrass_Stater, Illinoisan, Bay_Stater, Californian, Mississippian, South_Dakotan, Wisconsinite, Badger, Marylander, Ohioan, Buckeye, Puerto_Rican, Anglo-American, Arkansan, Arkansawyer, South_Carolinian, Texan, Vermonter, African-American, African_American, Afro-American, Black_American, Delawarean, Delawarian, Kansan, Coloradan, West_Virginian, Indianan, Hoosier, Bostonian, Idahoan, Franco-American, Arizonan, Arizonian, Oklahoman, Sooner, Yank, Yankee-Doodle, Connecticuter, Nisei, Iowan, Pennsylvanian, Keystone_Stater, Hawaiian, New_Hampshirite, Granite_Stater, Creole, Nevadan, Asian_American, Mainer, Down_Easter, Minnesotan, Gopher, Tory, Oregonian, Beaver, Louisianan, Louisianian, Nebraskan, Cornhusker, North_Dakotan, Tennessean, Volunteer, German_American, Virginian, Georgian, Utahan, Southerner, Montanan, Alaskan, Floridian, New_Yorker, Appalachian, Northerner, Washingtonian, Spanish_American, Hispanic_American, Hispanic, Alabaman, Alabamian, New_Jerseyan, New_Jerseyite, Garden_Stater, Missourian, Michigander, Wolverine, African_American_Vernacular_English, AAVE, African_American_English, Black_English, Black_English_Vernacular, Black_Vernacular, Black_Vernacular_English, Ebonics, South_American, Latin_American, Latino, Mesoamerican, North_American, West_Indian\n",
      "\n",
      "Word: generation\n",
      "  Synonyms: coevals, contemporaries, generation, genesis, multiplication, propagation\n",
      "  Hyponyms: youth_culture, peer_group, baby_boom, baby-boom_generation, generation_X, gen_X, posterity, biogenesis, biogeny\n",
      "\n",
      "Word: forebears\n",
      "  Synonyms: forebear, forbear\n",
      "  Hyponyms: great_grandparent, grandparent\n",
      "\n",
      "Word: revolution\n",
      "  Synonyms: revolution, rotation, gyration\n",
      "  Hyponyms: Cultural_Revolution, Great_Proletarian_Cultural_Revolution, green_revolution, counterrevolution, dextrorotation, clockwise_rotation, levorotation, counterclockwise_rotation, spin, axial_rotation, axial_motion, roll, orbital_rotation, orbital_motion\n",
      "\n",
      "Word: committed\n",
      "  Synonyms: perpetrate, commit, pull, give, dedicate, consecrate, devote, institutionalize, institutionalise, send, charge, entrust, intrust, trust, confide, invest, put, place, practice, committed, attached\n",
      "  Hyponyms: make, recommit, vow, consecrate, apply, rededicate, hospitalize, hospitalise, commend, obligate, consign, charge, speculate, job, fund, buy_into, roll_over, tie_up, shelter\n",
      "\n",
      "Word: powerful\n",
      "  Synonyms: powerful, knock-down, potent, brawny, hefty, muscular, sinewy, herculean, mighty, mightily, right\n",
      "  Hyponyms: \n",
      "\n",
      "Word: supporting\n",
      "  Synonyms: support, supporting, back_up, back, endorse, indorse, plump_for, plunk_for, hold, sustain, hold_up, confirm, corroborate, substantiate, affirm, subscribe, underpin, bear_out, defend, fend_for, patronize, patronise, patronage, keep_going, digest, endure, stick_out, stomach, bear, stand, tolerate, brook, abide, suffer, put_up, encouraging, load-bearing\n",
      "  Hyponyms: suspension, dangling, hanging, shoring, shoring_up, propping_up, promote, advance, boost, further, encourage, help, assist, aid, second, back, endorse, indorse, sponsor, patronize, patronise, shop, shop_at, buy_at, frequent, undergird, subsidize, subsidise, provide, bring_home_the_bacon, fund, see_through, champion, defend, guarantee, warrant, scaffold, pole, brace, underpin, truss, block, prop_up, prop, shore_up, shore, buoy, buoy_up, chock, bracket, carry, prove, demonstrate, establish, show, shew, validate, document, back_up, vouch, verify, apologize, apologise, excuse, justify, rationalize, rationalise, uphold, stand_up, stick_up, sit_out, bear_up, pay, accept, live_with, swallow, take_a_joke, stand_for, hold_still_for, take_lying_down\n",
      "\n",
      "Word: themselves\n",
      "  Synonyms: \n",
      "  Hyponyms: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "for word, _ in top_10_long_words:\n",
    "    synonyms = []\n",
    "    hyponyms = []\n",
    "    \n",
    "    for synset in wn.synsets(word):\n",
    "\n",
    "        for lemma in synset.lemma_names():\n",
    "            if lemma not in synonyms:\n",
    "                synonyms.append(lemma)\n",
    "        \n",
    "        for hypo in synset.hyponyms():\n",
    "            for lemma in hypo.lemma_names():\n",
    "                if lemma not in hyponyms:\n",
    "                    hyponyms.append(lemma)\n",
    "    \n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"  Synonyms: {', '.join(synonyms)}\")\n",
    "    print(f\"  Hyponyms: {', '.join(hyponyms)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Task 2.5: Reflect of the Results**##\n",
    "\n",
    "In this project, I learned many different techniques around usage and frequency of modal verbs. The two sources used were the Gutenberg Corpus and Kennedy's 1961 Inaugural Speech. Here are some of the notworthy observations I made while completing this assignemnt:\n",
    "\n",
    "- 1) Corpora libraries and their texts required different handling from regular text documents on your machine. For instance, a standard text file on my machine would need the file path, open it, read it, and then close it. This is because nltk handles file management in the background when you access them.\n",
    "\n",
    "- 2) When accessing text files, the best way to get all the words of the file into a structure (array, dictionary), is by using a splitter. In this case gutenberg.words() takes care of this in an easy manner, allow me to loop through each word in the array. \n",
    "\n",
    "- 3) I've witness the benefit of using arrays to dynamically create keys in a dictionary. This can be seen in the count_of_each_modal_in_file dictionary and others. \n",
    "\n",
    "- 4) When comparing strings, they are case sensitive, 'Can' is different that 'can' so a normalized format should be used. In this case, converting all words to lowercase to match the list_of_modals array.\n",
    "\n",
    "- 5) Conducting Modal Span Analysis to see the text with the highest and lowest frequencies of each modal helps determine not just difference in counts but also difference in writing styles.\n",
    "\n",
    "- 6) The use of WordNet was new to me as a programmer. Counting long words and storing them to an array allowed me to loop through the array and pass in each word to the wn.sysnet() function. This was able to return synonyms and hyponyms of each of the long words. I noticed that for words like 'powerful' and 'themselves' that it did not return any hypoymns. 'Themselves' also didnt return any synonyms. At first I thought this was error in the code but it turns out they are not in the hierarchy of WordNet. Not all code is perfect and just because something works, doesn't mean it can't be improved. \n",
    "\n",
    "Overall, I understand the importance of nltk for text processing to ensure it is efficient and logical. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
